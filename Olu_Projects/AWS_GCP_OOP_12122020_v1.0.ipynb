{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How the script works (Version 1.0):\n",
    "# Contains a Class(AWS) and sub-Class(GCP):\n",
    "# 1. Create both AWS and GCP buckets\n",
    "# 2. Gets and multiple CSV files for upload into AWS bucket \n",
    "# 3. Upload to bucket and Unzip for data analytics\n",
    "# 4. Import all csv files into Pandas df and merge as one large CSV file\n",
    "# 5. Optionally Delete AWS bucket\n",
    "# 6. Performance Analytics on whole file and output for further Analysis in GCP cloud\n",
    "# 7. Now transfers/upload large CSV into GCP Bucket created for customer pick-up via REST API\n",
    "# 8. Ingest csv Data into SQL DB for PowerBI feed\n",
    "# 8. Optionally Delete GCP bucket but check if bucket exists in a list b4 deletion\n",
    "\n",
    "# Next Target: \n",
    "# 1. download files on AWS & GCP via REST API\n",
    "# 2. Perform more Cleansing and Data Minging\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This works\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "import numpy as np\n",
    "import pyodbc\n",
    "from sqlalchemy import create_engine\n",
    "import urllib\n",
    "import boto3\n",
    "from boto3.session import Session\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "from google.cloud import storage\n",
    "import google.cloud.storage\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import getpass\n",
    "\n",
    "        \n",
    "class CloudConnect:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.create_zip_type = \"zip\"\n",
    "        self.source_zip_file_format = \".zip\"\n",
    "        self.source_dir = \"C:\\\\PythonTraining\\\\files\\\\Azurefiles\\\\mkt_data\"\n",
    "        self.source_zip_path_to_unpack = \"C:\\\\PythonTraining\\\\files\\\\AWS\\\\zip_files\\\\market_data_zip\"\n",
    "        self.unpacked_dest_dir = \"C:\\\\PythonTraining\\\\files\\\\AWS\\\\unpacked_files\"\n",
    "        self.zip_source_dir_path = \"C:\\\\PythonTraining\\\\files\\\\AWS\\\\zip_files\\\\\"\n",
    "        self.zip_source_dir_filename = \"market_data_zip.zip\"\n",
    "        self.filename = (self.zip_source_dir_path + self.zip_source_dir_filename)\n",
    "        self.zip_dir_to_unpack = \"C:\\\\PythonTraining\\\\files\\\\AWS\\\\zip_files\\\\\"\n",
    "        self.zip_filename_to_unpack =\"market_data_zip\"\n",
    "        self.zip_fullpath_to_unpack = (self.zip_dir_to_unpack + self.zip_filename_to_unpack)\n",
    "        self.concat_file_dir = \"C:\\\\PythonTraining\\\\files\\\\Azurefiles\\\\mkt_data\\\\*.csv\"\n",
    "        self.read_dir_files = glob.glob(self.concat_file_dir)\n",
    "        self.file_output_for_upload = \"C:\\\\PythonTraining\\\\files\\\\AWS\\\\merged_for_upload\\\\combined_mkt_data.csv\"\n",
    "        self.merged_files = pd.concat([pd.read_csv(file) for file in self.read_dir_files], ignore_index=True)\n",
    "        self.gcp_source_file_name = \"C:\\\\PythonTraining\\\\files\\\\AWS\\\\merged_for_upload\\\\combined_mkt_data.csv\"\n",
    "#         self.df: DataFrame = pd.read_csv(self.gcp_source_file_name) #fix this later\n",
    "    \n",
    "#     Create aws bucket\n",
    "    def create_aws_s3_bucket(self, aws_bucket_name: str):\n",
    "    \n",
    "        s3 = boto3.client(\"s3\")\n",
    "#         s3 = self.s3\n",
    "        session = boto3.session.Session()\n",
    "        current_region = session.region_name\n",
    "        s3.create_bucket(\n",
    "        Bucket=aws_bucket_name,\n",
    "        CreateBucketConfiguration={\"LocationConstraint\": current_region}\n",
    "        )\n",
    "\n",
    "    \n",
    "    def create_zip_files(self):\n",
    "        return shutil.make_archive(self.dest_zip_name, self.create_zip_type, self.source_dir)\n",
    "    \n",
    "#     Upload zip files to AWS Bucket\n",
    "    def upload_file_to_s3(self):\n",
    "#     import boto3\n",
    "        s3 = boto3.client('s3')\n",
    "        filename = self.filename\n",
    "        bucket_name = \"olu-bucket1\"\n",
    "        return s3.upload_file(filename, bucket_name, filename)    \n",
    "        \n",
    "        \n",
    "#     Unzip CSV files to work with\n",
    "    def unpack_zip_files_in_aws(self):\n",
    "        return shutil.unpack_archive((self.source_zip_path_to_unpack + self.source_zip_file_format), \n",
    "                                     self.source_zip_path_to_unpack)\n",
    "                \n",
    "#     Merge and Performance analysis on data\n",
    "    def df_import_and_concat_csv(self):\n",
    "        return self.merged_files.to_csv(self.file_output_for_upload)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def delete_bucket_with_boto3(bucket_name: str):\n",
    "        s3 = boto3.client(\"s3\")\n",
    "    \n",
    "        s3.delete_bucket(\n",
    "            Bucket=bucket_name\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# aws = CloudConnect(\"olu-bucket-for-data-upload12345\")\n",
    "# aws.create_aws_s3_bucket()\n",
    "aws = CloudConnect()\n",
    "# aws.create_aws_s3_bucket(\"olu-bucket1\")\n",
    "# aws.create_zip_files()\n",
    "# aws.upload_file_to_s3()\n",
    "# aws.unpack_zip_files_in_aws()\n",
    "# aws.df_import_and_concat_csv()\n",
    "# aws.delete_bucket_with_boto3(\"olu-bucket1\")\n",
    "\n",
    "#####################################################################\n",
    "    \n",
    "# send combined csv files to GCP\n",
    "# 1. Create GCP bucket and upload combined files\n",
    "# 2. Do some analytics\n",
    "# 3. Download and Insert into SQL DB (IAAS Assumed)\n",
    "# 4. Delete bucket\n",
    "\n",
    "\n",
    "class ConnectGcpCloud(CloudConnect):\n",
    "    \n",
    "    def __init__(self):\n",
    "        CloudConnect.__init__(self)\n",
    "        \n",
    "        self.key = getpass.getpass('enter key: ') #using getpass module to hide key\n",
    "    \n",
    "        self.PATH = os.path.join(os.getcwd(), self.key) \n",
    "        os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = self.PATH\n",
    "        \n",
    "    \n",
    "#     create google bucket\n",
    "    def create_gcp_bucket(self, gcp_bucket_name: str):\n",
    "        \"\"\"Create a new bucket in specific location with storage class\"\"\"\n",
    "        storage_client = storage.Client()\n",
    "\n",
    "        bucket = storage_client.bucket(gcp_bucket_name)\n",
    "        bucket.storage_class = \"COLDLINE\"\n",
    "        new_bucket = storage_client.create_bucket(bucket, location=\"europe-west2\")\n",
    "\n",
    "        print(\n",
    "            \"Created bucket {} in {} with storage class {}\".format(\n",
    "                new_bucket.name, new_bucket.location, new_bucket.storage_class\n",
    "            )\n",
    "        )\n",
    "        return new_bucket\n",
    "\n",
    "    \n",
    "    def upload_file_to_gcp_blob(self,gcp_bucket_name,gcp_destination_blob_name):\n",
    "        \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket(gcp_bucket_name)\n",
    "        blob = bucket.blob(gcp_destination_blob_name)\n",
    "\n",
    "        return blob.upload_from_filename(self.gcp_source_file_name)\n",
    "\n",
    "        print(\n",
    "            \"File {} uploaded to {}.\".format(\n",
    "                self.gcp_source_file_name, gcp_destination_blob_name\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "#     Check if bucket Exists before Upload\n",
    "    def list_gcp_buckets(self):\n",
    "        \n",
    "        \"\"\"Lists all buckets.\"\"\"\n",
    "        storage_client = storage.Client()\n",
    "        buckets = storage_client.list_buckets()\n",
    "\n",
    "        file_lst = [bucket.name for bucket in buckets]\n",
    "        return file_lst\n",
    "\n",
    "    \n",
    "    def delete_gcp_bucket(self, bucket_name):\n",
    "        \"\"\"Deletes a bucket. The bucket must be empty.\"\"\"\n",
    "        # bucket_name = \"your-bucket-name\"\n",
    "\n",
    "        storage_client = storage.Client()\n",
    "\n",
    "        bucket = storage_client.get_bucket(bucket_name)\n",
    "        bucket.delete()\n",
    "\n",
    "        print(\"Bucket {} deleted\".format(bucket.name))\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "# Keep your credentials well (sample My First Project-d4d25cc66bfc.json)\n",
    "gcp = ConnectGcpCloud()\n",
    "# gcp.create_gcp_bucket(\"olu-bucket122\")\n",
    "# gcp.upload_file_to_gcp_blob(\"olu-bucket111\", \"gcp_blob_doc_144\")\n",
    "# gcp.list_gcp_buckets()\n",
    "# gcp.delete_gcp_bucket(\"olu-bucket122\")\n",
    "# gcp.ingest_to_sql_server(df, \"market_data\") not needed again- new class created\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into sql table\n",
    "# Need to integrate inot the larger class\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyodbc\n",
    "from sqlalchemy import create_engine\n",
    "import urllib\n",
    "\n",
    "path = \"C:\\\\PythonTraining\\\\files\\\\AWS\\\\merged_for_upload\\\\combined_mkt_data.csv\"\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "def ingest_to_sql_server(df: pd.DataFrame, table_name: str):\n",
    "#     path = \"C:\\\\PythonTraining\\\\files\\\\AWS\\\\merged_for_upload\\\\combined_mkt_data.csv\"\n",
    "#     df = pd.read_csv(path)\n",
    "\n",
    "    params = urllib.parse.quote_plus(r'DRIVER={SQL+Server+Native+Client+11.0};SERVER=OLU;DATABASE=Python;Trusted_Connection=yes')\n",
    "    conn_str = 'mssql+pyodbc:///?odbc_connect={}'.format(params)\n",
    "#print(conn_str)\n",
    "    engine = create_engine(conn_str)\n",
    "    df.to_sql(table_name, conn_str, index=False, if_exists='replace')\n",
    "    \n",
    "ingest_to_sql_server(df, 'market_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
